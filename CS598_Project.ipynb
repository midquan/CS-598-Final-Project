{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAk6QqjqJRRN",
        "outputId": "0e049457-f6d9-48b9-cbb0-27747a35992d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyts in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.11/dist-packages (from pyts) (0.61.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.55.2->pyts) (0.44.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas matplotlib scikit-learn pyts torch tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beHavbV2J4Fw",
        "outputId": "d5b482dd-2cd3-4f6a-95e7-3d191e35b68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-06 11:34:14--  https://archive.ics.uci.edu/static/public/348/indoor+user+movement+prediction+from+rss+data.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘data/Indoor_User_Movement_Prediction.zip’\n",
            "\n",
            "data/Indoor_User_Mo     [   <=>              ] 209.99K   346KB/s    in 0.6s    \n",
            "\n",
            "2025-05-06 11:34:15 (346 KB/s) - ‘data/Indoor_User_Movement_Prediction.zip’ saved [215028]\n",
            "\n",
            "--2025-05-06 11:34:15--  https://archive.ics.uci.edu/static/public/256/daily+and+sports+activities.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘data/Daily_and_Sports_Activity.zip’\n",
            "\n",
            "data/Daily_and_Spor     [    <=>             ] 162.89M  17.4MB/s    in 11s     \n",
            "\n",
            "2025-05-06 11:34:26 (15.5 MB/s) - ‘data/Daily_and_Sports_Activity.zip’ saved [170800010]\n",
            "\n",
            "replace data/Indoor_User_Movement_Prediction/dataset/MovementAAL_RSS_1.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace data/Daily_and_Sports_Activity/data/a01/p1/s01.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Create directories\n",
        "!mkdir -p data/Indoor_User_Movement_Prediction\n",
        "!mkdir -p data/Daily_and_Sports_Activity\n",
        "\n",
        "# Download zip files from UCI (replace these URLs with the actual direct download links)\n",
        "!wget -O data/Indoor_User_Movement_Prediction.zip \"https://archive.ics.uci.edu/static/public/348/indoor+user+movement+prediction+from+rss+data.zip\"\n",
        "!wget -O data/Daily_and_Sports_Activity.zip \"https://archive.ics.uci.edu/static/public/256/daily+and+sports+activities.zip\"\n",
        "\n",
        "# Unzip files into the correct folders\n",
        "!unzip -q data/Indoor_User_Movement_Prediction.zip -d data/Indoor_User_Movement_Prediction\n",
        "!unzip -q data/Daily_and_Sports_Activity.zip -d data/Daily_and_Sports_Activity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YRDHolg3dpc2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# --- Logging setup for both file and console ---\n",
        "logger = logging.getLogger('dual_logger')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# File handler (will not log warnings)\n",
        "class NoWarningFileHandler(logging.FileHandler):\n",
        "    def emit(self, record):\n",
        "        if record.levelno != logging.WARNING:\n",
        "            super().emit(record)\n",
        "\n",
        "file_handler = NoWarningFileHandler('results_output.log')\n",
        "file_handler.setLevel(logging.INFO)\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Console handler (shows everything)\n",
        "console_handler = logging.StreamHandler(sys.__stdout__)\n",
        "console_handler.setLevel(logging.INFO)\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# --- Show each warning only once ---\n",
        "warnings.simplefilter('once')\n",
        "\n",
        "# --- Redirect print statements to logger.info ---\n",
        "class LoggerWriter:\n",
        "    def __init__(self, level_func):\n",
        "        self.level_func = level_func\n",
        "    def write(self, message):\n",
        "        message = message.rstrip()\n",
        "        if message:\n",
        "            self.level_func(message)\n",
        "    def flush(self):\n",
        "        pass\n",
        "\n",
        "sys.stdout = LoggerWriter(logger.info)\n",
        "sys.stderr = LoggerWriter(logger.error)\n",
        "\n",
        "# --- Suppress warnings from being logged to file ---\n",
        "def custom_showwarning(message, category, filename, lineno, file=None, line=None):\n",
        "    # Only print warnings to console, not to file\n",
        "    sys.__stderr__.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
        "\n",
        "warnings.showwarning = custom_showwarning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vmn3RJRtV1QT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Conv1D, BatchNormalization, Activation, GlobalAveragePooling1D, Add, Flatten, PReLU, MaxPooling1D, Lambda, Multiply\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import GroupNormalization\n",
        "\n",
        "def build_mlp(input_shape, nb_classes):\n",
        "    x = Input(shape=input_shape)\n",
        "    x_nb = Dense(64, activation='relu')(x)\n",
        "    x_nb = Dropout(0.2)(x_nb)\n",
        "    out = Dense(nb_classes, activation='softmax')(x_nb)\n",
        "    return x, out\n",
        "\n",
        "def build_lstm(input_shape, nb_classes):\n",
        "    x = Input(shape=input_shape)\n",
        "    x_nb = LSTM(64)(x)\n",
        "    x_nb = Dropout(0.2)(x_nb)\n",
        "    out = Dense(nb_classes, activation='softmax')(x_nb)\n",
        "    return x, out\n",
        "\n",
        "def build_fcn(input_shape, nb_classes):\n",
        "    x = Input(shape=input_shape)\n",
        "    conv_x = BatchNormalization()(x)\n",
        "    conv_x = Conv1D(128, kernel_size=8, padding='same')(conv_x)\n",
        "    conv_x = Activation('relu')(conv_x)\n",
        "    conv_x = Dropout(0.2)(conv_x)\n",
        "    full = GlobalAveragePooling1D()(conv_x)\n",
        "    out = Dense(nb_classes, activation='softmax')(full)\n",
        "    return x, out\n",
        "\n",
        "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
        "    x = Input(shape=input_shape)\n",
        "    conv_x = BatchNormalization()(x)\n",
        "    conv_x = Conv1D(n_feature_maps, kernel_size=8, padding='same')(conv_x)\n",
        "    conv_x = BatchNormalization()(conv_x)\n",
        "    conv_x = Activation('relu')(conv_x)\n",
        "    conv_y = Conv1D(n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
        "    conv_y = BatchNormalization()(conv_y)\n",
        "    conv_y = Activation('relu')(conv_y)\n",
        "    conv_z = Conv1D(n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "    conv_z = BatchNormalization()(conv_z)\n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = Conv1D(n_feature_maps, kernel_size=1, padding='same')(x)\n",
        "        shortcut_y = BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = BatchNormalization()(x)\n",
        "    y = Add()([shortcut_y, conv_z])\n",
        "    y = Activation('relu')(y)\n",
        "    x1 = y\n",
        "    conv_x = Conv1D(n_feature_maps * 2, kernel_size=8, padding='same')(x1)\n",
        "    conv_x = BatchNormalization()(conv_x)\n",
        "    conv_x = Activation('relu')(conv_x)\n",
        "    conv_y = Conv1D(n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "    conv_y = BatchNormalization()(conv_y)\n",
        "    conv_y = Activation('relu')(conv_y)\n",
        "    conv_z = Conv1D(n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "    conv_z = BatchNormalization()(conv_z)\n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps * 2)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = Conv1D(n_feature_maps * 2, kernel_size=1, padding='same')(x1)\n",
        "        shortcut_y = BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = BatchNormalization()(x1)\n",
        "    y = Add()([shortcut_y, conv_z])\n",
        "    y = Activation('relu')(y)\n",
        "    x1 = y\n",
        "    conv_x = Conv1D(n_feature_maps * 2, kernel_size=8, padding='same')(x1)\n",
        "    conv_x = BatchNormalization()(conv_x)\n",
        "    conv_x = Activation('relu')(conv_x)\n",
        "    conv_y = Conv1D(n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "    conv_y = BatchNormalization()(conv_y)\n",
        "    conv_y = Activation('relu')(conv_y)\n",
        "    conv_z = Conv1D(n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "    conv_z = BatchNormalization()(conv_z)\n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps * 2)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = Conv1D(n_feature_maps * 2, kernel_size=1, padding='same')(x1)\n",
        "        shortcut_y = BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = BatchNormalization()(x1)\n",
        "    y = Add()([shortcut_y, conv_z])\n",
        "    y = Activation('relu')(y)\n",
        "    full = GlobalAveragePooling1D()(y)\n",
        "    out = Dense(nb_classes, activation='softmax')(full)\n",
        "    return x, out\n",
        "\n",
        "def build_encoder(input_shape, nb_classes):\n",
        "    x = Input(input_shape)\n",
        "    conv1 = Conv1D(filters=128, kernel_size=5, strides=1, padding='same')(x)\n",
        "    conv1 = GroupNormalization(groups=-1, axis=-1)(conv1)\n",
        "    conv1 = PReLU(shared_axes=[1])(conv1)\n",
        "    conv1 = Dropout(rate=0.2)(conv1)\n",
        "    conv1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "    conv2 = Conv1D(filters=256, kernel_size=11, strides=1, padding='same')(conv1)\n",
        "    conv2 = GroupNormalization(groups=-1, axis=-1)(conv2)\n",
        "    conv2 = PReLU(shared_axes=[1])(conv2)\n",
        "    conv2 = Dropout(rate=0.2)(conv2)\n",
        "    conv2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "    conv3 = Conv1D(filters=512, kernel_size=21, strides=1, padding='same')(conv2)\n",
        "    conv3 = GroupNormalization(groups=-1, axis=-1)(conv3)\n",
        "    conv3 = PReLU(shared_axes=[1])(conv3)\n",
        "    conv3 = Dropout(rate=0.2)(conv3)\n",
        "    attention_data = Lambda(lambda x: x[:, :, :256])(conv3)\n",
        "    attention_softmax = Lambda(lambda x: x[:, :, 256:])(conv3)\n",
        "    attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
        "    multiply_layer = Multiply()([attention_softmax, attention_data])\n",
        "    dense_layer = Dense(units=256, activation='sigmoid')(multiply_layer)\n",
        "    dense_layer = GroupNormalization(groups=-1, axis=-1)(dense_layer)\n",
        "    flatten_layer = Flatten()(dense_layer)\n",
        "    out = Dense(units=nb_classes, activation='softmax')(flatten_layer)\n",
        "    return x, out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1eb5FZBEnJ8x"
      },
      "outputs": [],
      "source": [
        "from pyts.metrics import dtw, boss\n",
        "from scipy.stats import wasserstein_distance\n",
        "import numpy as np\n",
        "\n",
        "def boss_metric(x, y):\n",
        "    return boss(x, y)\n",
        "\n",
        "def dtw_classic(x, y):\n",
        "    return dtw(x, y, method='classic')\n",
        "\n",
        "def dtw_sakoechiba(x, y, window_size=0.5):\n",
        "    return dtw(x, y, method='sakoechiba', options={'window_size': window_size})\n",
        "\n",
        "def dtw_itakura(x, y, max_slope=1.5):\n",
        "    return dtw(x, y, method='itakura', options={'max_slope': max_slope})\n",
        "\n",
        "def dtw_multiscale(x, y, resolution=2):\n",
        "    return dtw(x, y, method='multiscale', options={'resolution': resolution})\n",
        "\n",
        "def dtw_fast(x, y, radius=1):\n",
        "    return dtw(x, y, method='fast', options={'radius': radius})\n",
        "\n",
        "def wasserstein_metric(x, y):\n",
        "    return wasserstein_distance(x, y)\n",
        "\n",
        "def cal_similarity(view1, view2, metric='boss'):\n",
        "    similarity_list = []\n",
        "    for i in range(view1.shape[0]):\n",
        "        x = np.squeeze(view1[i])\n",
        "        y = np.squeeze(view2[i])\n",
        "        if metric == 'boss':\n",
        "            similarity_list.append(boss(x, y))\n",
        "        elif metric == 'dtw_classic':\n",
        "            similarity_list.append(dtw_classic(x, y))\n",
        "        elif metric == 'dtw_sakoechiba':\n",
        "            similarity_list.append(dtw_sakoechiba(x, y, window_size=0.5))\n",
        "        elif metric == 'dtw_itakura':\n",
        "            similarity_list.append(dtw_itakura(x, y, max_slope=1.5))\n",
        "        elif metric == 'dtw_multiscale':\n",
        "            similarity_list.append(dtw_multiscale(x, y, resolution=2))\n",
        "        elif metric == 'dtw_fast':\n",
        "            similarity_list.append(dtw_fast(x, y, radius=1))\n",
        "        elif metric == 'wasserstein':\n",
        "            similarity_list.append(wasserstein_metric(x, y))\n",
        "        else:\n",
        "            print('other metric not implement yet.')\n",
        "    return np.array(similarity_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r8K8SpLMnJ6Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC, TopKCategoricalAccuracy\n",
        "import os\n",
        "\n",
        "# --- TPU strategy setup ---\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print(\"Running on TPU!\")\n",
        "except Exception as e:\n",
        "    strategy = tf.distribute.get_strategy()  # Default strategy (CPU/GPU)\n",
        "    print(\"TPU not found, running on CPU/GPU.\")\n",
        "\n",
        "def print_metrics(history):\n",
        "    for metric_name, values in history.history.items():\n",
        "        print(f\"{metric_name}: {values[-1]:.4f}\")\n",
        "\n",
        "def prepare_views(data_train, data_test):\n",
        "    views = {}\n",
        "    for i in range(data_train.shape[2]):\n",
        "        views[f'view{i}_train'] = data_train[:,:,i].reshape(data_train.shape[0], data_train.shape[1], 1)\n",
        "        views[f'view{i}_test'] = data_test[:,:,i].reshape(data_test.shape[0], data_test.shape[1], 1)\n",
        "    return views\n",
        "\n",
        "def calculate_similarities(views, source_idx, metric):\n",
        "    similarities = {}\n",
        "    n_views = len([k for k in views if '_train' in k])\n",
        "    for i in range(n_views):\n",
        "        if i != source_idx:\n",
        "            similarities[f'similarity{source_idx}{i}'] = cal_similarity(\n",
        "                views[f'view{source_idx}_train'],\n",
        "                views[f'view{i}_train'],\n",
        "                metric\n",
        "            )\n",
        "    return similarities\n",
        "\n",
        "def calculate_weights(similarities, source_idx):\n",
        "    kde_models = {}\n",
        "    weights = {}\n",
        "    weight_all = 0\n",
        "    target_indices = [int(k[len(f'similarity{source_idx}') :]) for k in similarities.keys()]\n",
        "    for i in target_indices:\n",
        "        kde_key = f'kde{source_idx}{i}'\n",
        "        sim_key = f'similarity{source_idx}{i}'\n",
        "        sim_data = similarities[sim_key]\n",
        "        if sim_data.shape[0] == 0:\n",
        "            print(f\"Warning: No samples for {sim_key}, skipping KDE fit.\")\n",
        "            continue\n",
        "        kde_models[kde_key] = KernelDensity(kernel='gaussian', bandwidth=7.8).fit(\n",
        "            sim_data.reshape(sim_data.flatten().shape[0], 1)\n",
        "        )\n",
        "        weight_key = f'weight{source_idx}{i}'\n",
        "        weights[weight_key] = np.mean(kde_models[kde_key].sample(10, random_state=0), axis=0)[0]\n",
        "        weight_all += weights[weight_key]\n",
        "    return kde_models, weights, weight_all\n",
        "\n",
        "def train_model(model_type, views, source_idx, target_train, target_test, nb_classes, transfer_type='no_transfer', weights=None, weight_all=None):\n",
        "    with strategy.scope():\n",
        "        if model_type == 'fcn':\n",
        "            x, y = build_fcn(views[f'view0_train'].shape[1:], nb_classes)\n",
        "        else: # lstm\n",
        "            x, y = build_lstm(views[f'view0_train'].shape[1:], nb_classes)\n",
        "        model = keras.models.Model(inputs=x, outputs=y)\n",
        "        adam = Adam(learning_rate=0.005)\n",
        "        model.compile(\n",
        "            loss='categorical_crossentropy',\n",
        "            optimizer=adam,\n",
        "            metrics=['accuracy', Precision(), Recall(), AUC(), TopKCategoricalAccuracy(k=5)],\n",
        "            steps_per_execution=50  # improves TPU throughput[1]\n",
        "        )\n",
        "\n",
        "    n_views = len([k for k in views if '_train' in k])\n",
        "    target_indices = [i for i in range(n_views) if i != source_idx]\n",
        "    if transfer_type in ['naive_transfer', 'weighted_transfer']:\n",
        "        for i in target_indices:\n",
        "            if transfer_type == 'naive_transfer':\n",
        "                epochs = 30\n",
        "            else: # weighted_transfer\n",
        "                weight_key = f'weight{source_idx}{i}'\n",
        "                if weights is not None and weight_key in weights and weight_all:\n",
        "                    epochs = int(30 * 7 * weights[weight_key] / weight_all) + 1\n",
        "                else:\n",
        "                    print(f\"Warning: {weight_key} not found in weights. Skipping this view.\")\n",
        "                    continue\n",
        "            model.fit(\n",
        "                views[f'view{i}_train'],\n",
        "                to_categorical(target_train, nb_classes),\n",
        "                epochs=epochs,\n",
        "                batch_size=128,\n",
        "                validation_data=(views[f'view{i}_test'], to_categorical(target_test, nb_classes)),\n",
        "                verbose=0\n",
        "            )\n",
        "    epochs = 40 if transfer_type in ['no_transfer', 'weighted_transfer'] else 30\n",
        "    history = model.fit(\n",
        "        views[f'view{source_idx}_train'],\n",
        "        to_categorical(target_train, nb_classes),\n",
        "        epochs=epochs,\n",
        "        batch_size=128,\n",
        "        validation_data=(views[f'view{source_idx}_test'], to_categorical(target_test, nb_classes)),\n",
        "        verbose=0\n",
        "    )\n",
        "    return history\n",
        "\n",
        "def run_training(data_train, data_test, target_train, target_test, source_idx=0, model_types=['fcn', 'lstm'], nb_classes=2):\n",
        "    metrics = ['boss', 'dtw_classic', 'dtw_sakoechiba', 'dtw_itakura', 'dtw_multiscale', 'wasserstein']\n",
        "    results = {model_type: {} for model_type in model_types}\n",
        "    views = prepare_views(data_train, data_test)\n",
        "    n_views = len([k for k in views if '_train' in k])\n",
        "    for metric in metrics:\n",
        "        mode_name = f'{metric}+kde'\n",
        "        print(f\"\\n===== Processing {mode_name} =====\")\n",
        "        similarities = calculate_similarities(views, source_idx, metric)\n",
        "        kde_models, weights, weight_all = calculate_weights(similarities, source_idx)\n",
        "        for model_type in model_types:\n",
        "            results[model_type][mode_name] = {}\n",
        "            print(f'*** {model_type.upper()} - {mode_name} - No transfer learning ***')\n",
        "            history = train_model(model_type, views, source_idx, target_train, target_test, nb_classes)\n",
        "            results[model_type][mode_name]['No_transfer'] = {\n",
        "                metric: history.history[metric] for metric in history.history if metric.startswith('val_')\n",
        "            }\n",
        "            print(f\"Final accuracy: {history.history['val_accuracy'][-1]}\")\n",
        "            print_metrics(history)\n",
        "            print(f'*** {model_type.upper()} - {mode_name} - Naive transfer learning ***')\n",
        "            history = train_model(model_type, views, source_idx, target_train, target_test, nb_classes, transfer_type='naive_transfer')\n",
        "            results[model_type][mode_name]['Naive_Transfer'] = {\n",
        "                metric: history.history[metric] for metric in history.history if metric.startswith('val_')\n",
        "            }\n",
        "            print(f\"Final accuracy: {history.history['val_accuracy'][-1]}\")\n",
        "            print_metrics(history)\n",
        "            print(f'*** {model_type.upper()} - {mode_name} - Weighted transfer learning ***')\n",
        "            history = train_model(model_type, views, source_idx, target_train, target_test, nb_classes, transfer_type='weighted_transfer', weights=weights, weight_all=weight_all)\n",
        "            results[model_type][mode_name]['Weighted_Transfer'] = {\n",
        "                metric: history.history[metric] for metric in history.history if metric.startswith('val_')\n",
        "            }\n",
        "            print(f\"Final accuracy: {history.history['val_accuracy'][-1]}\")\n",
        "            print_metrics(history)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm4qZNP3nJ4Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "def load_sensor_data(data_root):\n",
        "    segments, labels, metadata = [], [], []\n",
        "    for activity in sorted(os.listdir(data_root)):\n",
        "        activity_path = os.path.join(data_root, activity)\n",
        "        if not os.path.isdir(activity_path): continue\n",
        "        for person in sorted(os.listdir(activity_path)):\n",
        "            person_path = os.path.join(activity_path, person)\n",
        "            if not os.path.isdir(person_path): continue\n",
        "            for session in sorted(os.listdir(person_path)):\n",
        "                if session.endswith('.txt'):\n",
        "                    path = os.path.join(person_path, session)\n",
        "                    raw = np.loadtxt(path, delimiter=\",\")\n",
        "                    segments.append(raw)  # shape (125, 45)\n",
        "                    labels.append(activity)\n",
        "                    metadata.append((activity, person, session))\n",
        "    return segments, labels, metadata\n",
        "\n",
        "def scale_segments(segment_list):\n",
        "    return [MinMaxScaler(feature_range=(-1, 1)).fit_transform(seg) for seg in segment_list]\n",
        "\n",
        "def segment_windows(segments, labels, metadata):\n",
        "    X, y, meta_out = [], [], []\n",
        "    for segment, label, meta in zip(segments, labels, metadata):\n",
        "        X.append(segment.T)  # (45, 125)\n",
        "        y.append(label)\n",
        "        meta_out.append(meta)\n",
        "    return np.stack(X), np.array(y), meta_out\n",
        "\n",
        "def split_body_domains(X):\n",
        "    return {\n",
        "        'torso': X[:, 0:9, :],\n",
        "        'right_arm': X[:, 9:18, :],\n",
        "        'left_arm': X[:, 18:27, :],\n",
        "        'right_leg': X[:, 27:36, :],\n",
        "        'left_leg': X[:, 36:45, :]\n",
        "    }\n",
        "\n",
        "# Load, scale, and prepare data\n",
        "data_root = \"data/Daily_and_Sports_Activity/data/\"\n",
        "segments, labels, metadata = load_sensor_data(data_root)\n",
        "segments = scale_segments(segments)\n",
        "X, y, meta = segment_windows(segments, labels, metadata)\n",
        "le = LabelEncoder()\n",
        "y_int = le.fit_transform(y)\n",
        "domains = split_body_domains(X)\n",
        "view = domains['torso']  # (samples, 9, 125)\n",
        "X_view = np.transpose(view, (0, 2, 1))  # (samples, 125, 9)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_view, y_int, test_size=0.2, random_state=42, stratify=y_int)\n",
        "data_train, data_test, targets_train, targets_test = X_train, X_test, y_train, y_test\n",
        "nb_classes = len(np.unique(targets_train))\n",
        "\n",
        "# --- Run training loop ---\n",
        "results = run_training(\n",
        "    data_train, data_test, targets_train, targets_test,\n",
        "    source_idx=0, model_types=['fcn', 'lstm'], nb_classes=nb_classes\n",
        ")\n",
        "print(\"\\n=== Final Validation Accuracies ===\")\n",
        "for model_type in results:\n",
        "    print(f\"\\nModel: {model_type.upper()}\")\n",
        "    for mode_name in results[model_type]:\n",
        "        print(f\" {mode_name}:\")\n",
        "        for strategy in results[model_type][mode_name]:\n",
        "            metrics_dict = results[model_type][mode_name][strategy]\n",
        "            if metrics_dict:\n",
        "                for metric in sorted(metrics_dict):\n",
        "                  curve = metrics_dict[metric]\n",
        "                  print(f\"  {strategy} - {metric}: {curve[-1]:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {strategy}: No data\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}